import os, sys
from dotenv import load_dotenv
import requests
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss


load_dotenv()
# Cloudflare AI –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
ACCOUNT_ID = os.getenv('CLOUDFLARE_ACCOUNT_ID', 'your-account-id')
AUTH_TOKEN = os.getenv('CLOUDFLARE_AUTH_TOKEN', 'your-token')
CLOUDFLARE_URL = f"https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/meta/llama-3.3-70b-instruct-fp8-fast"

IPCC_DOCS = [
    "Climate change has caused widespread adverse impacts to nature and people.",
    "Human-induced climate change is affecting weather and climate extremes globally.",
    "Key risks include water scarcity, food insecurity, coastal flooding, and heat mortality.",
    "Rising sea levels and coastal flooding are increasing.",
    "Climate change has reduced food security and hindered economic growth.",
    "Ecosystems face widespread impacts from climate change.",
    "Cities face increasing risks from heat stress and coastal flooding.",
    "Climate-driven food price increases have been observed.",
    "3.3-3.6 billion people live in contexts highly vulnerable to climate change.",
    "Heatwaves have increased in intensity causing increased mortality.",
]


def generate_variants(query):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è 3 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ Cloudflare AI (Llama 3.3)"""
    prompt = f"""Generate exactly 3 different ways to ask this question. 
Keep the same meaning but use different words and phrasing.
Output ONLY the 3 questions, numbered 1-3, nothing else.

Original question: {query}

3 variants:"""

    response = requests.post(
        CLOUDFLARE_URL,
        headers={"Authorization": f"Bearer {AUTH_TOKEN}"},
        json={
            "messages": [
                {"role": "system", "content": "You are a query reformulation expert. Generate diverse query variants."},
                {"role": "user", "content": prompt}
            ]
        },
        timeout=30
    )
    
    result = response.json()
    
    # Cloudflare –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ result['result']['response']
    if 'result' in result and 'response' in result['result']:
        text = result['result']['response']
    else:
        raise Exception(f"Unexpected API response format: {result}")
    
    # –ü–∞—Ä—Å–∏–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–∑ –æ—Ç–≤–µ—Ç–∞
    variants = []
    for line in text.split('\n'):
        line = line.strip().lstrip('0123456789.-) ').strip('"\'')
        if len(line) > 10 and line not in variants:
            variants.append(line)
    
    return variants[:3]


def search_docs(queries, documents, k=5):
    """–ü–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —Å –ø–æ–º–æ—â—å—é embeddings –∏ FAISS"""
    print("   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ embeddings...")
    encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    print("   –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    doc_embeddings = encoder.encode(documents)
    doc_embeddings = np.array(doc_embeddings, dtype='float32')
    faiss.normalize_L2(doc_embeddings)
    
    index = faiss.IndexFlatIP(doc_embeddings.shape[1])
    index.add(doc_embeddings)
    
    # –ü–æ–∏—Å–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    all_found = set()
    
    print(f"   –ü–æ–∏—Å–∫ –ø–æ {len(queries)} –∑–∞–ø—Ä–æ—Å–∞–º...")
    for query in queries:
        query_emb = encoder.encode([query])
        query_emb = np.array(query_emb, dtype='float32')
        faiss.normalize_L2(query_emb)
        
        scores, indices = index.search(query_emb, k)
        all_found.update(indices[0].tolist())
    
    return all_found


def calculate_similarity(original, variants):
    """–°—á–∏—Ç–∞–µ–º similarity –º–µ–∂–¥—É –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º –∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏"""
    encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    orig_emb = encoder.encode([original])[0]
    var_embs = encoder.encode(variants)
    
    scores = []
    for var_emb in var_embs:
        sim = np.dot(orig_emb, var_emb) / (np.linalg.norm(orig_emb) * np.linalg.norm(var_emb))
        scores.append(float(sim))
    
    return scores


def main():
    query = ' '.join(sys.argv[1:])
    
    print("\n" + "="*70)
    print("AI QUERY OPTIMIZER")
    print("Cloudflare AI (Llama 3.3) + Sentence Transformers + FAISS")
    print("="*70)
    print(f"\nüìù –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query}'")
    
    # 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —á–µ—Ä–µ–∑ Cloudflare AI
    print("\n[–®–∞–≥ 1/4] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ Cloudflare AI (Llama 3.3)...")
    try:
        variants = generate_variants(query)
        print(f"   ‚úì –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(variants)} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤")
    except Exception as e:
        print(f"   ‚úó –û—à–∏–±–∫–∞: {e}")
        print("\n‚ö†Ô∏è  –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:")
        print("   - CLOUDFLARE_ACCOUNT_ID")
        print("   - CLOUDFLARE_AUTH_TOKEN")
        return
    
    # 2. –°—á–∏—Ç–∞–µ–º similarity scores
    print("\n[–®–∞–≥ 2/4] –†–∞—Å—á–µ—Ç similarity scores –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏...")
    similarities = calculate_similarity(query, variants)
    print(f"   ‚úì –†–∞—Å—Å—á–∏—Ç–∞–Ω—ã –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ –¥–∏—Å—Ç–∞–Ω—Ü–∏–∏")
    
    # 3. Baseline retrieval (—Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å)
    print("\n[–®–∞–≥ 3/4] Baseline retrieval (1 –∑–∞–ø—Ä–æ—Å)...")
    baseline_docs = search_docs([query], IPCC_DOCS, k=5)
    print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ {len(baseline_docs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # 4. Optimized retrieval (–æ—Ä–∏–≥–∏–Ω–∞–ª + 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞)
    print("\n[–®–∞–≥ 4/4] Optimized retrieval (4 –∑–∞–ø—Ä–æ—Å–∞: –æ—Ä–∏–≥–∏–Ω–∞–ª + –≤–∞—Ä–∏–∞–Ω—Ç—ã)...")
    optimized_docs = search_docs([query] + variants, IPCC_DOCS, k=5)
    print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ {len(optimized_docs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    recall_improvement = (len(optimized_docs) - len(baseline_docs)) / len(baseline_docs) * 100
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "="*70)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("="*70)
    
    print("\nüîÑ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:")
    for i, (variant, score) in enumerate(zip(variants, similarities), 1):
        print(f"\n  {i}. [Similarity: {score:.3f}]")
        print(f"     {variant}")
    
    print("\n" + "-"*70)
    print("üìä Retrieval —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   Baseline (1 query):     {len(baseline_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print(f"   Optimized (4 queries):  {len(optimized_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print(f"   –ù–æ–≤—ã—Ö –Ω–∞–π–¥–µ–Ω–æ:          {len(optimized_docs) - len(baseline_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print(f"   Recall improvement:     {recall_improvement:+.1f}%")
    
    print("\n" + "-"*70)
    if recall_improvement >= 20:
        print("‚úÖ –¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞: recall —É–ª—É—á—à–µ–Ω–∏–µ ‚â•20% (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ PRD)")
    else:
        print(f"‚ö†Ô∏è  Recall +{recall_improvement:.1f}% (—Ü–µ–ª—å: ‚â•20%)")
        print("   üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å –∏–ª–∏ —É–≤–µ–ª–∏—á—å—Ç–µ k")
    
    print("\n" + "="*70)
    print("\n‚ú® –ì–æ—Ç–æ–≤–æ! Multi-query retrieval —Ä–∞–±–æ—Ç–∞–µ—Ç.")
    print("üí° –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ RAG: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤—Å–µ 4 –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n")


if __name__ == '__main__':
    main()